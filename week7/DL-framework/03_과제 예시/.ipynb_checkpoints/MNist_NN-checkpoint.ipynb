{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 : \n",
    "- Fashion-MNIST Dataset으로 MLP를 자유롭게 구현해보세요.\n",
    "- PyTorch, TensorFlow 선택 자유\n",
    "- Data 파일을 따로 드리지 않습니다. Dataset과 관련된 모듈을 활용하여 직접 해보시길 바랍니다.\n",
    "- 공개된 코드를 사용하셔도 좋습니다. 이 경우 출처를 밝혀 주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 다운받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "# Download and load the training data\n",
    "trainDataset = datasets.FashionMNIST('../F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testDataset = datasets.FashionMNIST('../F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 데이터 확인\n",
    "print(trainDataset.train_data.size())\n",
    "print(trainDataset.train_labels.size())\n",
    "\n",
    "idx=2\n",
    "plt.imshow(trainDataset.train_data[idx,:,:].numpy(), cmap='gray')\n",
    "plt.title('%i' % trainDataset.train_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,  22, 118,  24,   0,   0,\n",
       "           0,   0,   0,  48,  88,   5,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  12, 100, 212, 205, 185, 179,\n",
       "         173, 186, 193, 221, 142,  85,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  85,  76, 199, 225, 248,\n",
       "         255, 238, 226, 157,  68,  80,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  91,  69,  91, 201, 218,\n",
       "         225, 209, 158,  61,  93,  72,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  79,  89,  61,  59,  87,\n",
       "         108,  75,  56,  76,  97,  73,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  75,  89,  80,  80,  67,\n",
       "          63,  73,  83,  80,  96,  72,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  77,  88,  77,  80,  83,\n",
       "          83,  83,  83,  81,  95,  76,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  89,  96,  80,  83,  81,\n",
       "          84,  85,  85,  85,  97,  84,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  93,  97,  81,  85,  84,\n",
       "          85,  87,  88,  84,  99,  87,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  95,  87,  84,  87,  88,\n",
       "          85,  87,  87,  84,  92,  87,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  97,  87,  87,  85,  88,\n",
       "          87,  87,  87,  88,  85, 107,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  17, 100,  88,  87,  87,  88,\n",
       "          87,  87,  85,  89,  77, 118,   8,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  10,  93,  87,  87,  87,  87,\n",
       "          87,  88,  87,  89,  80, 103,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   9,  96,  87,  87,  87,  87,\n",
       "          87,  88,  87,  88,  87, 103,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  12,  96,  85,  87,  87,  87,\n",
       "          85,  87,  87,  88,  89, 100,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  20,  95,  84,  88,  85,  87,\n",
       "          88,  88,  88,  89,  88,  99,   8,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  21,  96,  85,  87,  85,  88,\n",
       "          88,  88,  88,  89,  89,  99,  10,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  24,  96,  85,  87,  85,  87,\n",
       "          88,  88,  89,  88,  91, 102,  14,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  25,  93,  84,  88,  87,  87,\n",
       "          87,  87,  87,  89,  91, 103,  29,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  95,  85,  88,  88,  87,\n",
       "          87,  87,  87,  89,  88, 102,  37,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  34,  96,  88,  87,  87,  87,\n",
       "          87,  87,  87,  85,  85,  97,  38,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  40,  96,  87,  85,  87,  87,\n",
       "          87,  87,  87,  85,  84,  92,  49,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  46,  95,  83,  84,  87,  87,\n",
       "          87,  87,  87,  87,  84,  87,  84,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  72,  95,  85,  84,  85,  88,\n",
       "          87,  87,  89,  87,  85,  83,  63,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  64, 100,  84,  87,  88,  85,\n",
       "          88,  88,  84,  87,  83,  95,  53,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  10, 102, 100,  91,  91,  89,\n",
       "          85,  84,  84,  87, 108, 106,  14,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   8,  73,  93, 104, 107,\n",
       "         103, 103, 106, 102,  75,  10,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,  18,  42,\n",
       "          57,  56,  32,   8,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader.dataset.data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# nn은 레이어간 weight 공유, F는 단순히 연산만 \n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__() # 상속받아 설계해야 한다.\n",
    "        self.dense1 = nn.Linear(784, 512) # 첫번째 히든레이어 설정 (N, H1)\n",
    "        self.dense1_bn = torch.nn.BatchNorm1d(512) # 첫번째 히든레이어 후 배치노멀라이제이션 적용\n",
    "        self.dense2 = nn.Linear(512, 256) # 두번째 히든레이어 설정 (H1, H2)\n",
    "        self.dense3 = nn.Linear(256, 256) # 세번째 히든레이어 설정 (H2, H3)\n",
    "#         self.dense4 = nn.Linear(512, 10) # 마지막 레이어 (H3, O)\n",
    "        self.dense4 = nn.Linear(256, 10) # 마지막 레이어 (H3, O)\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,784) # input값을 넣어 reshape 한다음 foward 과정\n",
    "        output = self.dense1(output)\n",
    "        output = self.dense1_bn(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.dense2(output)\n",
    "        output = self.dense3(output)\n",
    "        output = F.dropout(output, training=self.training)\n",
    "        output = self.dense4(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 설정\n",
    "# use gpu -> .cuda() : model과 Variable에 쓰임\n",
    "model = Network().to(device)\n",
    "\n",
    "# 손실함수 : nn.CrossEntropyLoss()의 경우 기본적으로 LogSoftmax()가 내장\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화 함수 : parameters를 통해 parameter 할당가능 ,lr : learning_rate\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for batch_idx, (data, label) in enumerate(trainLoader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        output = model(data) #forward 단계 실행 => output계산\n",
    "        loss = criterion(output, label) # 모델에서 나온 output과 label을 이용해 loss 계산\n",
    "\n",
    "        optimizer.zero_grad() # 갱신할 Variable들에 대한 모든 변화도를 0으로 만듬\n",
    "        loss.backward() # 역전파 단계 실행. 모델의 Variable들에 대한 loss의 변화도를 계산합니다.\n",
    "        optimizer.step() # 가중치 갱신\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1] # 텐서 배열의 최대값이 들어가있는 label반환\n",
    "        train_acc += pred.eq(label.data.view_as(pred)).sum() # pred와 라벨 비교\n",
    "        \n",
    "        ### LOGGING\n",
    "#         if not batch_idx % 50:\n",
    "#             print ('Epoch: %03d | Batch %03d/%03d | Cost: %.4f' \n",
    "#                    %(epoch, batch_idx, \n",
    "#                      len(trainLoader), loss))\n",
    "            \n",
    "    train_loss /= len(trainLoader.dataset)\n",
    "\n",
    "    print('Train Epoch: {} Average loss: {:.4f} Accuracy : {:.4f}%)'.format(epoch, train_loss, 100. * train_acc / len(trainLoader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for data, target in testLoader:\n",
    "        # volatile=True no use backprob\n",
    "        data, target = data, target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        test_acc += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(testLoader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {:.3f}%)'.format(test_loss, 100. * test_acc / len(testLoader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.0255 Accuracy : 79.0000%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0082, Accuracy: 81.000%)\n",
      "Train Epoch: 2 Average loss: 0.0071 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0071, Accuracy: 84.000%)\n",
      "Train Epoch: 3 Average loss: 0.0065 Accuracy : 85.0000%)\n",
      "Test set: Average loss: 0.0075, Accuracy: 84.000%)\n",
      "Train Epoch: 4 Average loss: 0.0062 Accuracy : 86.0000%)\n",
      "Test set: Average loss: 0.0076, Accuracy: 83.000%)\n",
      "Train Epoch: 5 Average loss: 0.0060 Accuracy : 86.0000%)\n",
      "Test set: Average loss: 0.0072, Accuracy: 84.000%)\n",
      "Train Epoch: 6 Average loss: 0.0059 Accuracy : 86.0000%)\n",
      "Test set: Average loss: 0.0068, Accuracy: 85.000%)\n",
      "Train Epoch: 7 Average loss: 0.0059 Accuracy : 87.0000%)\n",
      "Test set: Average loss: 0.0075, Accuracy: 84.000%)\n",
      "Train Epoch: 8 Average loss: 0.0056 Accuracy : 87.0000%)\n",
      "Test set: Average loss: 0.0085, Accuracy: 83.000%)\n",
      "Train Epoch: 9 Average loss: 0.0057 Accuracy : 87.0000%)\n",
      "Test set: Average loss: 0.0073, Accuracy: 85.000%)\n",
      "Train Epoch: 10 Average loss: 0.0054 Accuracy : 87.0000%)\n",
      "Test set: Average loss: 0.0079, Accuracy: 84.000%)\n",
      "Wall time: 4min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n4분 46초 정도 소요\\ntrain acc : 79, test acc : 79\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 모델 초기화후 돌리기\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "'''\n",
    "4분 46초 정도 소요\n",
    "relu 적용 전 train acc : 79, test acc : 79\n",
    "4분 55초 정도 소요\n",
    "relu 적용 후 train acc : 87, test acc : 84\n",
    "활성화 함수를 적용한 것만으로도 결과가 좋아짐을 볼 수 있다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출처\n",
    "### https://mjdeeplearning.tistory.com/81 (가장 기초부터 설명되어 있어서 따라해 보았다.)\n",
    "### https://www.yceffort.kr/2019/01/27/pytorch-2-multi-perceptron(2)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BATCHNORM 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.0073 Accuracy : 83.0000%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0071, Accuracy: 83.000%)\n",
      "Train Epoch: 2 Average loss: 0.0055 Accuracy : 87.0000%)\n",
      "Test set: Average loss: 0.0068, Accuracy: 84.000%)\n",
      "Train Epoch: 3 Average loss: 0.0050 Accuracy : 88.0000%)\n",
      "Test set: Average loss: 0.0066, Accuracy: 85.000%)\n",
      "Train Epoch: 4 Average loss: 0.0046 Accuracy : 89.0000%)\n",
      "Test set: Average loss: 0.0070, Accuracy: 84.000%)\n",
      "Train Epoch: 5 Average loss: 0.0043 Accuracy : 89.0000%)\n",
      "Test set: Average loss: 0.0070, Accuracy: 84.000%)\n",
      "Train Epoch: 6 Average loss: 0.0041 Accuracy : 90.0000%)\n",
      "Test set: Average loss: 0.0066, Accuracy: 86.000%)\n",
      "Train Epoch: 7 Average loss: 0.0039 Accuracy : 90.0000%)\n",
      "Test set: Average loss: 0.0053, Accuracy: 88.000%)\n",
      "Train Epoch: 8 Average loss: 0.0037 Accuracy : 91.0000%)\n",
      "Test set: Average loss: 0.0061, Accuracy: 86.000%)\n",
      "Train Epoch: 9 Average loss: 0.0035 Accuracy : 91.0000%)\n",
      "Test set: Average loss: 0.0064, Accuracy: 87.000%)\n",
      "Train Epoch: 10 Average loss: 0.0034 Accuracy : 91.0000%)\n",
      "Test set: Average loss: 0.0061, Accuracy: 87.000%)\n",
      "Wall time: 6min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n4분 46초 정도 소요\\ntrain acc : 79, test acc : 79\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    \n",
    "'''\n",
    "6분 44초 정도 소요\n",
    "train acc : 91, test acc : 87\n",
    "정규화는 필수 인 걸로!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 층을 2개 더 늘린 경우 (주석풀면서 실행해야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.1490 Accuracy : 74.0000%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0103, Accuracy: 77.000%)\n",
      "Train Epoch: 2 Average loss: 0.0097 Accuracy : 81.0000%)\n",
      "Test set: Average loss: 0.0153, Accuracy: 68.000%)\n",
      "Train Epoch: 3 Average loss: 0.0081 Accuracy : 83.0000%)\n",
      "Test set: Average loss: 0.0086, Accuracy: 80.000%)\n",
      "Train Epoch: 4 Average loss: 0.0079 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0091, Accuracy: 81.000%)\n",
      "Train Epoch: 5 Average loss: 0.0077 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0072, Accuracy: 84.000%)\n",
      "Train Epoch: 6 Average loss: 0.0078 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0067, Accuracy: 85.000%)\n",
      "Train Epoch: 7 Average loss: 0.0080 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0097, Accuracy: 83.000%)\n",
      "Train Epoch: 8 Average loss: 0.0068 Accuracy : 85.0000%)\n",
      "Test set: Average loss: 0.0069, Accuracy: 84.000%)\n",
      "Train Epoch: 9 Average loss: 0.0069 Accuracy : 86.0000%)\n",
      "Test set: Average loss: 0.0070, Accuracy: 85.000%)\n",
      "Train Epoch: 10 Average loss: 0.0080 Accuracy : 85.0000%)\n",
      "Test set: Average loss: 0.0092, Accuracy: 82.000%)\n",
      "Wall time: 7min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n2분정도 소요되었고\\n히든레이어를 3층을 쌓아서 돌려 보았는데 확실히 시간이 더 걸린다.\\ntrain acc : 97 test acc : 97 \\n무조건 츠응ㄹ 많이 쌓는다고 좋은 거은 아닌 것 같다.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "'''\n",
    "7분 13 소요되었고\n",
    "히든레이어를 3층을 쌓아서 돌려 보았는데 확실히 시간이 더 걸린다.\n",
    "train acc : 85 test acc : 82 \n",
    "무조건 층을 많이 쌓는다고 좋은 거은 아닌 것 같다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 층을 2개 쌓은것에 DROP OUT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.1103 Accuracy : 73.0000%)\n",
      "Test set: Average loss: 0.0096, Accuracy: 79.000%)\n",
      "Train Epoch: 2 Average loss: 0.0099 Accuracy : 79.0000%)\n",
      "Test set: Average loss: 0.0092, Accuracy: 81.000%)\n",
      "Train Epoch: 3 Average loss: 0.0086 Accuracy : 82.0000%)\n",
      "Test set: Average loss: 0.0101, Accuracy: 78.000%)\n",
      "Train Epoch: 4 Average loss: 0.0084 Accuracy : 82.0000%)\n",
      "Test set: Average loss: 0.0091, Accuracy: 82.000%)\n",
      "Train Epoch: 5 Average loss: 0.0080 Accuracy : 83.0000%)\n",
      "Test set: Average loss: 0.0093, Accuracy: 77.000%)\n",
      "Train Epoch: 6 Average loss: 0.0081 Accuracy : 83.0000%)\n",
      "Test set: Average loss: 0.0081, Accuracy: 83.000%)\n",
      "Train Epoch: 7 Average loss: 0.0076 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0078, Accuracy: 85.000%)\n",
      "Train Epoch: 8 Average loss: 0.0075 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0151, Accuracy: 74.000%)\n",
      "Train Epoch: 9 Average loss: 0.0074 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0127, Accuracy: 75.000%)\n",
      "Train Epoch: 10 Average loss: 0.0074 Accuracy : 84.0000%)\n",
      "Test set: Average loss: 0.0079, Accuracy: 84.000%)\n",
      "Wall time: 7min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n7분 정도 소요되었고 \\ndropout이 학습시간을 증가시켜 나온 결과이고\\ndropout이 일반화 성능을 증가시켜\\ntest acc 95이 train 94더 높게 나옴을 볼 수 있다.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "'''\n",
    "7분 30초 정도 소요되었고 \n",
    "dropout이 학습시간을 증가시켜 나온 결과이고\n",
    "dropout이 일반화 성능을 증가시켜\n",
    "test acc 84 train 84 보다 높게 나옴을 볼 수 있다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
