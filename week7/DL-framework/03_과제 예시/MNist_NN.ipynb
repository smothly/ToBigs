{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 : \n",
    "- Fashion-MNIST Dataset으로 MLP를 자유롭게 구현해보세요.\n",
    "- PyTorch, TensorFlow 선택 자유\n",
    "- Data 파일을 따로 드리지 않습니다. Dataset과 관련된 모듈을 활용하여 직접 해보시길 바랍니다.\n",
    "- 공개된 코드를 사용하셔도 좋습니다. 이 경우 출처를 밝혀 주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 다운받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '4')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANqklEQVR4nO3db4wc9X3H8c/HEEsUYmTzxzlhC7uRQY0qYyKDCkTgysRy/cTkAREWFFdFHCpBaqRUKqIPgmpVgoqkyoMS6QLIprikkcyBFYUkllVBK4F1Z+SC/2CbWsY5+2QHURQjE1LDtw92Lj3M7ex5d3Zn777vl3Ta3fnu7Hw18se/mZ3d/TkiBGD2m1N3AwB6g7ADSRB2IAnCDiRB2IEkCDuQBGEHkiDsaMr2Mtu/tf1c3b2gc4QdZf5Z0kjdTaAahB1Tsn2XpA8k7ay7F1SDsONzbM+T9PeSvlN3L6gOYcdUNkl6OiJ+VXcjqM6FdTeA/mJ7haTbJV1fdy+oFmHHuVZJWiLpmG1JukTSBba/EhFfrbEvdMh8xRWT2f4DSfMmLfobNcL/VxHx61qaQiUY2fEZEXFG0pmJx7Y/lPRbgj7zMbIDSfBuPJAEYQeSIOxAEoQdSKKn78bb5t1AoMsiwlMt72hkt73W9kHb79h+uJPXAtBdbV96s32BpEOSvi5pTI2vQm6IiP0l6zCyA13WjZH9RknvRMSRiPidpB9LWt/B6wHook7CfpWkyd+KGiuWfYbtQdujtkc72BaADnXyBt1UhwqfO0yPiCFJQxKH8UCdOhnZxyQtnvR4kaQTnbUDoFs6CfuIpGW2l9qeK+kuSduraQtA1do+jI+Is7YfkvQLSRdIeiYi9lXWGYBK9fRbb5yzA93XlQ/VAJg5CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii7SmbgX63evXqprWtW7eWrnvbbbeV1g8ePNhWT3XqKOy2j0o6LekTSWcjYmUVTQGoXhUj+59GxHsVvA6ALuKcHUii07CHpF/a3m17cKon2B60PWp7tMNtAehAp4fxt0TECdtXStph++2IeHXyEyJiSNKQJNmODrcHoE0djewRcaK4PSVpWNKNVTQFoHpth932xba/OHFf0hpJe6tqDEC1OjmMXyhp2PbE6/xrRPy8kq664NZbby2tX3bZZaX14eHhKttBD9xwww1NayMjIz3spD+0HfaIOCLpugp7AdBFXHoDkiDsQBKEHUiCsANJEHYgiTRfcV21alVpfdmyZaV1Lr31nzlzyseqpUuXNq1dffXVpesWl5RnFUZ2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUgizXX2e++9t7T+2muv9agTVGVgYKC0fv/99zetPffcc6Xrvv3222311M8Y2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiTTX2Vt99xkzz1NPPdX2uocPH66wk5mBBABJEHYgCcIOJEHYgSQIO5AEYQeSIOxAErPmOvvy5ctL6wsXLuxRJ+iVSy+9tO11d+zYUWEnM0PLkd32M7ZP2d47adkC2ztsHy5u53e3TQCdms5h/GZJa89Z9rCknRGxTNLO4jGAPtYy7BHxqqT3z1m8XtKW4v4WSXdU3BeAirV7zr4wIsYlKSLGbV/Z7Im2ByUNtrkdABXp+ht0ETEkaUiSbEe3twdgau1eejtpe0CSittT1bUEoBvaDft2SRuL+xslvVRNOwC6peVhvO3nJa2SdLntMUnflfSYpJ/Yvk/SMUl3drPJ6Vi3bl1p/aKLLupRJ6hKq89GlM2/3srx48fbXnemahn2iNjQpLS64l4AdBEflwWSIOxAEoQdSIKwA0kQdiCJWfMV12uvvbaj9fft21dRJ6jKE088UVpvdWnu0KFDTWunT59uq6eZjJEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5KYNdfZOzUyMlJ3CzPSvHnzSutr1577W6X/75577ildd82aNW31NGHTpk1Nax988EFHrz0TMbIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJcZy8sWLCgtm1fd911pXXbpfXbb7+9aW3RokWl686dO7e0fvfdd5fW58wpHy8++uijprVdu3aVrvvxxx+X1i+8sPyf7+7du0vr2TCyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjojebczu2saefPLJ0voDDzxQWm/1/eZjx46dd0/TtXz58tJ6q+vsZ8+ebVo7c+ZM6br79+8vrbe6Fj46Olpaf+WVV5rWTp48Wbru2NhYaX3+/Pml9VafIZitImLKfzAtR3bbz9g+ZXvvpGWP2j5ue0/xVz45OoDaTecwfrOkqX5u5J8iYkXx97Nq2wJQtZZhj4hXJb3fg14AdFEnb9A9ZPvN4jC/6cmT7UHbo7bLT+4AdFW7Yf+hpC9LWiFpXNL3mj0xIoYiYmVErGxzWwAq0FbYI+JkRHwSEZ9K+pGkG6ttC0DV2gq77YFJD78haW+z5wLoDy2/z277eUmrJF1ue0zSdyWtsr1CUkg6Kqn8InYPPPjgg6X1d999t7R+8803V9nOeWl1Df/FF18srR84cKBp7fXXX2+rp14YHBwsrV9xxRWl9SNHjlTZzqzXMuwRsWGKxU93oRcAXcTHZYEkCDuQBGEHkiDsQBKEHUgizU9JP/7443W3gHOsXr26o/W3bdtWUSc5MLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJprrNj9hkeHq67hRmFkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSmM6UzYslPSvpS5I+lTQUET+wvUDSv0laosa0zd+MiP/pXqvIxnZp/Zprrimt9/N01XWYzsh+VtJ3IuKPJP2JpG/Z/oqkhyXtjIhlknYWjwH0qZZhj4jxiHijuH9a0gFJV0laL2lL8bQtku7oVpMAOnde5+y2l0i6XtIuSQsjYlxq/Icg6cqqmwNQnWn/Bp3tSyRtk/TtiPhNq/OpSesNShpsrz0AVZnWyG77C2oEfWtEvFAsPml7oKgPSDo11boRMRQRKyNiZRUNA2hPy7C7MYQ/LelARHx/Umm7pI3F/Y2SXqq+PQBVmc5h/C2S/lzSW7b3FMsekfSYpJ/Yvk/SMUl3dqdFZBURpfU5c/iYyPloGfaI+E9JzU7QO5tgG0DP8F8jkARhB5Ig7EAShB1IgrADSRB2IAmmbMaMddNNN5XWN2/e3JtGZghGdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Iguvs6FvT/ekzTA8jO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXV21Obll18urd95J1MRVImRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeScKs5sG0vlvSspC9J+lTSUET8wPajku6X9OviqY9ExM9avFb5xgB0LCKm/CGA6YR9QNJARLxh+4uSdku6Q9I3JX0YEU9MtwnCDnRfs7C3/ARdRIxLGi/un7Z9QNJV1bYHoNvO65zd9hJJ10vaVSx6yPabtp+xPb/JOoO2R22PdtQpgI60PIz//RPtSyS9IukfIuIF2wslvScpJG1S41D/L1u8BofxQJe1fc4uSba/IOmnkn4REd+for5E0k8j4o9bvA5hB7qsWdhbHsa78ROfT0s6MDnoxRt3E74haW+nTQLonum8G/81Sf8h6S01Lr1J0iOSNkhaocZh/FFJDxRv5pW9FiM70GUdHcZXhbAD3df2YTyA2YGwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRK+nbH5P0ruTHl9eLOtH/dpbv/Yl0Vu7quzt6maFnn6f/XMbt0cjYmVtDZTo1976tS+J3trVq944jAeSIOxAEnWHfajm7Zfp1976tS+J3trVk95qPWcH0Dt1j+wAeoSwA0nUEnbba20ftP2O7Yfr6KEZ20dtv2V7T93z0xVz6J2yvXfSsgW2d9g+XNxOOcdeTb09avt4se/22F5XU2+Lbf+77QO299n+62J5rfuupK+e7Leen7PbvkDSIUlflzQmaUTShojY39NGmrB9VNLKiKj9Axi2b5X0oaRnJ6bWsv2Pkt6PiMeK/yjnR8Tf9klvj+o8p/HuUm/Nphn/C9W476qc/rwddYzsN0p6JyKORMTvJP1Y0voa+uh7EfGqpPfPWbxe0pbi/hY1/rH0XJPe+kJEjEfEG8X905Imphmvdd+V9NUTdYT9Kkm/mvR4TP0133tI+qXt3bYH625mCgsnptkqbq+suZ9ztZzGu5fOmWa8b/ZdO9Ofd6qOsE81NU0/Xf+7JSK+KunPJH2rOFzF9PxQ0pfVmANwXNL36mymmGZ8m6RvR8Rv6uxlsin66sl+qyPsY5IWT3q8SNKJGvqYUkScKG5PSRpW47Sjn5ycmEG3uD1Vcz+/FxEnI+KTiPhU0o9U474rphnfJmlrRLxQLK59303VV6/2Wx1hH5G0zPZS23Ml3SVpew19fI7ti4s3TmT7Yklr1H9TUW+XtLG4v1HSSzX28hn9Mo13s2nGVfO+q33684jo+Z+kdWq8I//fkv6ujh6a9PWHkv6r+NtXd2+SnlfjsO5/1Tgiuk/SZZJ2Sjpc3C7oo97+RY2pvd9UI1gDNfX2NTVODd+UtKf4W1f3vivpqyf7jY/LAknwCTogCcIOJEHYgSQIO5AEYQeSIOxAEoQdSOL/ACC+Ag+DlSY3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainDataset = datasets.MNIST('../mnist_data/',\n",
    "                             download=True,\n",
    "                             train=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(), # image to Tensor 기본으로 0과 1로 scailing 해준다.\n",
    "#                                  transforms.Normalize((0.5, 0.5), (0.5, 0.5)) # image, label\n",
    "                             ])) \n",
    "\n",
    "testDataset = datasets.MNIST(\"../mnist_data/\", \n",
    "                             download=False,\n",
    "                             train=False,\n",
    "                             transform= transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "#                                transforms.Normalize((0.5, 0.5), (0.5, 0.5))\n",
    "                           ]))\n",
    "\n",
    "# 데이터 확인\n",
    "print(trainDataset.train_data.size())\n",
    "print(trainDataset.train_labels.size())\n",
    "\n",
    "idx=2\n",
    "plt.imshow(trainDataset.train_data[idx,:,:].numpy(), cmap='gray')\n",
    "plt.title('%i' % trainDataset.train_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "batch_size = 64\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "\n",
    "testLoader = torch.utils.data.DataLoader(testDataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,  67, 232,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  62,  81,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0, 120, 180,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 126, 163,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   2, 153, 210,  40,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 220, 163,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,  27, 254, 162,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 222, 163,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0, 183, 254, 125,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,  46, 245, 163,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0, 198, 254,  56,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0, 120, 254, 163,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,  23, 231, 254,  29,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0, 159, 254, 120,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0, 163, 254, 216,  16,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0, 159, 254,  67,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  14,  86, 178, 248, 254,  91,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0, 159, 254,  85,   0,   0,   0,  47,  49, 116, 144, 150,\n",
       "         241, 243, 234, 179, 241, 252,  40,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0, 150, 253, 237, 207, 207, 207, 253, 254, 250, 240, 198,\n",
       "         143,  91,  28,   5, 233, 250,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 119, 177, 177, 177, 177, 177,  98,  56,   0,   0,\n",
       "           0,   0,   0, 102, 254, 220,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 169, 254, 137,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 169, 254,  57,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 169, 254,  57,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 169, 255,  94,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 169, 254,  96,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 169, 254, 153,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 169, 255, 153,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,  96, 254, 153,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader.dataset.data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# nn은 레이어간 weight 공유, F는 단순히 연산만 \n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__() # 상속받아 설계해야 한다.\n",
    "        self.dense1 = nn.Linear(784, 512) # 첫번째 히든레이어 설정 (N, H1)\n",
    "        self.dense1_bn = torch.nn.BatchNorm1d(512) # 첫번째 히든레이어 후 배치노멀라이제이션 적용\n",
    "        self.dense2 = nn.Linear(512, 256) # 두번째 히든레이어 설정 (H1, H2)\n",
    "        self.dense3 = nn.Linear(256, 256) # 세번째 히든레이어 설정 (H2, H3)\n",
    "#         self.dense4 = nn.Linear(512, 10) # 마지막 레이어 (H3, O)\n",
    "        self.dense4 = nn.Linear(256, 10) # 마지막 레이어 (H3, O)\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,784) # input값을 넣어 reshape 한다음 foward 과정\n",
    "        output = self.dense1(output)\n",
    "        output = self.dense1_bn(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.dense2(output)\n",
    "        output = self.dense3(output)\n",
    "        output = F.dropout(output, training=self.training)\n",
    "        output = self.dense4(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 설정\n",
    "# use gpu -> .cuda() : model과 Variable에 쓰임\n",
    "model = Network().to(device)\n",
    "\n",
    "# 손실함수 : nn.CrossEntropyLoss()의 경우 기본적으로 LogSoftmax()가 내장\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화 함수 : parameters를 통해 parameter 할당가능 ,lr : learning_rate\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train(epoch):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for batch_idx, (data, label) in enumerate(trainLoader):\n",
    "        data, label = Variable(data).to(device), Variable(label).to(device)\n",
    "        output = model(data) #forward 단계 실행 => output계산\n",
    "        loss = criterion(output, label) # 모델에서 나온 output과 label을 이용해 loss 계산\n",
    "\n",
    "        optimizer.zero_grad() # 갱신할 Variable들에 대한 모든 변화도를 0으로 만듬\n",
    "        loss.backward() # 역전파 단계 실행. 모델의 Variable들에 대한 loss의 변화도를 계산합니다.\n",
    "        optimizer.step() # 가중치 갱신\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1] # 텐서 배열의 최대값이 들어가있는 label반환\n",
    "        train_acc += pred.eq(label.data.view_as(pred)).sum() # pred와 라벨 비교\n",
    "        \n",
    "        ### LOGGING\n",
    "#         if not batch_idx % 50:\n",
    "#             print ('Epoch: %03d | Batch %03d/%03d | Cost: %.4f' \n",
    "#                    %(epoch, batch_idx, \n",
    "#                      len(trainLoader), loss))\n",
    "            \n",
    "    train_loss /= len(trainLoader.dataset)\n",
    "\n",
    "    print('Train Epoch: {} Average loss: {:.4f} Accuracy : {:.4f}%)'.format(epoch, train_loss, 100. * train_acc / len(trainLoader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for data, target in testLoader:\n",
    "        # volatile=True no use backprob\n",
    "        data, target = Variable(data, volatile=True).to(device), Variable(target).to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        test_acc += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(testLoader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {:.3f}%)'.format(test_loss, 100. * test_acc / len(testLoader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.0060 Accuracy : 93.0000%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daier/anaconda3/envs/testEnv/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0030, Accuracy: 94.000%)\n",
      "Train Epoch: 2 Average loss: 0.0022 Accuracy : 96.0000%)\n",
      "Test set: Average loss: 0.0023, Accuracy: 96.000%)\n",
      "Train Epoch: 3 Average loss: 0.0018 Accuracy : 96.0000%)\n",
      "Test set: Average loss: 0.0023, Accuracy: 96.000%)\n",
      "Train Epoch: 4 Average loss: 0.0016 Accuracy : 97.0000%)\n",
      "Test set: Average loss: 0.0028, Accuracy: 96.000%)\n",
      "Train Epoch: 5 Average loss: 0.0014 Accuracy : 97.0000%)\n",
      "Test set: Average loss: 0.0028, Accuracy: 96.000%)\n",
      "Train Epoch: 6 Average loss: 0.0014 Accuracy : 97.0000%)\n",
      "Test set: Average loss: 0.0029, Accuracy: 96.000%)\n",
      "Train Epoch: 7 Average loss: 0.0013 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0042, Accuracy: 96.000%)\n",
      "Train Epoch: 8 Average loss: 0.0012 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 96.000%)\n",
      "Train Epoch: 9 Average loss: 0.0011 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 97.000%)\n",
      "Train Epoch: 10 Average loss: 0.0011 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0038, Accuracy: 96.000%)\n",
      "CPU times: user 1min 29s, sys: 1.89 s, total: 1min 30s\n",
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n따로 테스트 하는 경우에 넣지는 않았지만, relu 적용안한 것보다 relu적용한 것이 더 정확도가 높게 나온다.\\n1분30초 정도 걸리고 train데이터 정확도는 98, test의 경우는 97정도 나온다.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 모델 초기화후 돌리기\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "'''\n",
    "1분30초 정도 걸리고\n",
    "relu 적용 전 train데이터 정확도는 88, test의 경우는 85정도 나온다.\n",
    "relu 적용 휴 train데이터 정확도는 98, test의 경우는 97정도 나온다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출처\n",
    "### https://mjdeeplearning.tistory.com/81 (가장 기초부터 설명되어 있어서 따라해 보았다.)\n",
    "### https://www.yceffort.kr/2019/01/27/pytorch-2-multi-perceptron(2)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BATCHNORM 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.0034 Accuracy : 94.0000%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daier/anaconda3/envs/testEnv/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0019, Accuracy: 96.000%)\n",
      "Train Epoch: 2 Average loss: 0.0015 Accuracy : 97.0000%)\n",
      "Test set: Average loss: 0.0015, Accuracy: 96.000%)\n",
      "Train Epoch: 3 Average loss: 0.0011 Accuracy : 97.0000%)\n",
      "Test set: Average loss: 0.0013, Accuracy: 97.000%)\n",
      "Train Epoch: 4 Average loss: 0.0008 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0011, Accuracy: 97.000%)\n",
      "Train Epoch: 5 Average loss: 0.0007 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0014, Accuracy: 97.000%)\n",
      "Train Epoch: 6 Average loss: 0.0006 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0015, Accuracy: 97.000%)\n",
      "Train Epoch: 7 Average loss: 0.0006 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0013, Accuracy: 98.000%)\n",
      "Train Epoch: 8 Average loss: 0.0006 Accuracy : 98.0000%)\n",
      "Test set: Average loss: 0.0016, Accuracy: 97.000%)\n",
      "Train Epoch: 9 Average loss: 0.0005 Accuracy : 99.0000%)\n",
      "Test set: Average loss: 0.0015, Accuracy: 97.000%)\n",
      "Train Epoch: 10 Average loss: 0.0005 Accuracy : 99.0000%)\n",
      "Test set: Average loss: 0.0015, Accuracy: 98.000%)\n",
      "CPU times: user 1min 54s, sys: 1.97 s, total: 1min 56s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    \n",
    "'''\n",
    "1분 40초 정도 소요\n",
    "첫번째 epoch부터 정확도 92 -> 94로 뛴것을 볼 수 있다.\n",
    "train acc : 99, test acc : 98\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 층을 2개 더 늘린 경우 (주석풀면서 실행해야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.0825 Accuracy : 86.0000%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daier/anaconda3/envs/testEnv/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0112, Accuracy: 81.000%)\n",
      "Train Epoch: 2 Average loss: 0.0039 Accuracy : 93.0000%)\n",
      "Test set: Average loss: 0.0024, Accuracy: 96.000%)\n",
      "Train Epoch: 3 Average loss: 0.0030 Accuracy : 95.0000%)\n",
      "Test set: Average loss: 0.0025, Accuracy: 96.000%)\n",
      "Train Epoch: 4 Average loss: 0.0026 Accuracy : 95.0000%)\n",
      "Test set: Average loss: 0.0021, Accuracy: 96.000%)\n",
      "Train Epoch: 5 Average loss: 0.0026 Accuracy : 96.0000%)\n",
      "Test set: Average loss: 0.0025, Accuracy: 96.000%)\n",
      "Train Epoch: 6 Average loss: 0.0022 Accuracy : 96.0000%)\n",
      "Test set: Average loss: 0.0019, Accuracy: 97.000%)\n",
      "Train Epoch: 7 Average loss: 0.0022 Accuracy : 96.0000%)\n",
      "Test set: Average loss: 0.0022, Accuracy: 96.000%)\n",
      "Train Epoch: 8 Average loss: 0.0023 Accuracy : 96.0000%)\n",
      "Test set: Average loss: 0.0028, Accuracy: 96.000%)\n",
      "Train Epoch: 9 Average loss: 0.0020 Accuracy : 97.0000%)\n",
      "Test set: Average loss: 0.0029, Accuracy: 97.000%)\n",
      "Train Epoch: 10 Average loss: 0.0021 Accuracy : 97.0000%)\n",
      "Test set: Average loss: 0.0025, Accuracy: 97.000%)\n",
      "CPU times: user 2min 21s, sys: 3.73 s, total: 2min 25s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "'''\n",
    "2분정도 소요되었고\n",
    "히든레이어를 3층을 쌓아서 돌려 보았는데 확실히 시간이 더 걸린다.\n",
    "train acc : 97 test acc : 97 \n",
    "무조건 츠응ㄹ 많이 쌓는다고 좋은 거은 아닌 것 같다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 층을 2개 쌓은것에 DROP OUT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.1909 Accuracy : 88.0000%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daier/anaconda3/envs/testEnv/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0056, Accuracy: 91.000%)\n",
      "Train Epoch: 2 Average loss: 0.0065 Accuracy : 91.0000%)\n",
      "Test set: Average loss: 0.0161, Accuracy: 84.000%)\n",
      "Train Epoch: 3 Average loss: 0.0059 Accuracy : 92.0000%)\n",
      "Test set: Average loss: 0.0047, Accuracy: 92.000%)\n",
      "Train Epoch: 4 Average loss: 0.0053 Accuracy : 92.0000%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 95.000%)\n",
      "Train Epoch: 5 Average loss: 0.0052 Accuracy : 93.0000%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 95.000%)\n",
      "Train Epoch: 6 Average loss: 0.0049 Accuracy : 93.0000%)\n",
      "Test set: Average loss: 0.0051, Accuracy: 93.000%)\n",
      "Train Epoch: 7 Average loss: 0.0048 Accuracy : 93.0000%)\n",
      "Test set: Average loss: 0.0052, Accuracy: 94.000%)\n",
      "Train Epoch: 8 Average loss: 0.0048 Accuracy : 93.0000%)\n",
      "Test set: Average loss: 0.0061, Accuracy: 92.000%)\n",
      "Train Epoch: 9 Average loss: 0.0049 Accuracy : 93.0000%)\n",
      "Test set: Average loss: 0.0057, Accuracy: 94.000%)\n",
      "Train Epoch: 10 Average loss: 0.0047 Accuracy : 94.0000%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 95.000%)\n",
      "CPU times: user 2min 41s, sys: 4.05 s, total: 2min 45s\n",
      "Wall time: 2min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n3분 20초 정도 소요되었고 \\ndropout이 학습시간을 증가시켜 나온 결과이고\\ndropout이 일반화 성능을 증가시켜\\ntest88이 train81 더 높게 나옴을 볼 수 있다.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "'''\n",
    "3분 정도 소요되었고 \n",
    "dropout이 학습시간을 증가시켜 나온 결과이고\n",
    "dropout이 일반화 성능을 증가시켜\n",
    "test acc 95이 train 94더 높게 나옴을 볼 수 있다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
